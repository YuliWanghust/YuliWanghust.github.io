
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN">
<html>
<head>
<title>Yuli Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"/>
<link rel="stylesheet" media="screen" href="stylesheets/stylesheet2.css"/>
</head>

<body>

<div id="info">
<h1 id="name">Yuli Wang</h1>
<img src="images/me-4.jpg"  hspace = 10 class = "rounded-big"/>
<p>
Graduate Student<br/>
<a href="https://www.bme.jhu.edu/">Biomedical Engineerig</a><br/>
<a href="https://www.jhu.edu/">Johns Hopkins University</a>
</p>

<p>
Email: ywang687 (at) jhu.edu<br/>
ResearchGate: <a href="https://www.researchgate.net/profile/Yuli_Wang7">@Yuli_Wang</a><br/>
Google Scholar: <a href="https://scholar.google.com/citations?user=wkheqT4AAAAJ&hl=en">Yuli Wang</a><br/>
</p>


<!-- <p> 
<em>Office Hours</em><br/>
By appointment
</p>
 -->
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "https://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-34471827-3");
pageTracker._trackPageview();
} catch(err) {}
</script>


<!-- <tr>
    <td valign="top">2015.02 </td>
	<td valign="top">
	InSilicoVA package released to CRAN
	</td> 
</tr> -->

</table>

 
</div>

<div id="main">
<h1>
<a class="nav" href="index.html">about</a> &nbsp;&nbsp;&nbsp;
<a class="nav" href="publication.html">publications</a> &nbsp;&nbsp;&nbsp;
<a class="nav" href="teaching.html">projects</a> &nbsp;&nbsp;&nbsp;
<a class="nav" href="CV_Yuli.pdf">vita</a> &nbsp;&nbsp;&nbsp;
<a class="nav" href="misc.html">misc</a> &nbsp;&nbsp;&nbsp;
</h1>

</br>


<!-- </ul> -->
<!-- ------------------------------------------------ -->
<br>


<h6 style="padding-top: 3px; padding-bottom: 1px;" id = "projects">Projects</h6>
<table width="100%" cellpadding="0" cellspacing="0" border-collapse="collapse">

<!-- ============================================================= -->
<tr padding-top="0">
    <td width="100%" valign="left" >    
        <div class="entry">
            <div>
                <img src="images/Workflow_nph.png" border="0" height = 220pt class="img-1"/></a>
            </div>
        <p>
               <b> Brain parcellation using deep learning and probability maps:</b> We present a 3D CNN with the U-Net architecture and residual blocks for brain ventricle segmentation in magnetic resonance images. We incorporate probability maps to improve network robustness. We conducted experiments over three cohorts: healthy controls, NPH patients, and NPH patients post surgical intervention with shunt placement. Our method produced results that are significantly better than the state-of-the-art alternatives and was successful with data that has shunt artifacts.
            [<i>Papers:</i> 
                <a href="https://livejohnshopkins-my.sharepoint.com/:b:/r/personal/ywang687_jh_edu/Documents/4_Personal/1_website/1_Publication/SPIE_Ventricle_segmentation.pdf?csf=1&web=1&e=bJkRta">SPIE Medical Imaging'23</a>;
            ]
        </p>
         
       </div>
    </td>
</tr>

<!-- ============================================================= -->
<tr padding-top="0">
    <td width="100%" valign="left" >    
        <div class="entry">
            <div>
                <img src="images/Workflow_slo.png" border="0" height = 220pt class="img-1"/></a>
            </div>
        <p>
               <b> Image registration using deep learning method:</b> This work reports a deep-learning based registration algorithm that aligns scanning laser ophthalmoscopy retinal images collected from a longitudinal pre-clinical animal study. We address the problem of determining correspondences between two retinal images in agreement with a geometric model such as an homography or thin-plate spline transformation, and estimating its parameters. The contributions of this work are two-fold. First, we propose a convolutional neural network architecture for retinal image registration based on geometric models. The architecture is based on three main components that mimic the standard steps of feature extraction, matching and model parameter estimation, while being trainable end-to-end. Second, we demonstrate that the network parameters can be trained from synthetically generated imagery without the need for manual annotation and that our matching layer significantly increases generalization capabilities to never-seen-before images.
            [<i>Papers:</i> 
                <a href="https://livejohnshopkins-my.sharepoint.com/:b:/r/personal/ywang687_jh_edu/Documents/4_Personal/1_website/1_Publication/SPIE_SLO_registration.pdf?csf=1&web=1&e=wk1XtG">SPIE Medical Imaging'23</a>;
            ]
        </p>
         
       </div>
    </td>
</tr>

<!-- ============================================================= -->
<tr padding-top="0">
    <td width="100%" valign="left" >	
        <div class="entry">
            <div>
                <img src="images/hnpet1.png" border="0" height = 220pt class="img-1"/></a>
            </div>
        <p>
               <b> A two-panel head and neck dedicated PET system:</b> We studied the performance of a two-panel positron emission tomography (PET) system dedicated to head and neck cancer (HNC). The goal of HNC treatment is not only to improve patient survival rate but also to preserve organ function and an HNC dedicated PET system with superior performance will help oncologists make better treatment plans. We will pursue the design, development, optimization, characterization, and validation of a dedicated head and neck PET scanner. Compared with a commercial whole-body PET system (GE Discovery MI), the proposed dedicated system shows better performance in terms of noise equivalent count rate, photon coincidence sensitivity, spatial resolution, and lesion visualization. The system will be integrated into a transportable stage and will be designed not to interfere with the conventional workflow of the WB-PET scan procedure.
            [<i>Papers:</i> 
               	<a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11595/1159510/Back-end-readout-electronic-design-and-initial-results--a/10.1117/12.2576598.short">SPIE Medical Imaging'20</a>;
            <i> Code:</i> 
                <a href="https://github.com/slaclab/ucsc-hn">SLAC-HN</a>
            ]
        </p>
         
	   </div>
    </td>
</tr>
<!-- ============================================================= -->
<tr padding-top="0">
    <td width="100%" valign="left" >	
        <div class="entry">
            <div>
                <img src="images/Simulation.png" border="0" height = 220pt class="img-1"/></a>
            </div>
        <p>
               <b> Optical property modulation method for PET :</b> Using conventional scintillation detection, the fundamental limit in positron emission tomography (PET) time resolution is strongly dependent on the inherent temporal variances generated during the scintillation process, yielding an intrinsic physical limit for the coincidence time resolution of around 100 ps. On the other hand, modulation mechanisms of the optical properties of a material exploited in the optical telecommunications industry can be orders of magnitude faster to sub-picosecond to femtosecond. In this work, we first completed the theoretical calculation of the optical modulation based single high energy photon detection using pyPENELOPE. We also borrow from the concept of optics pump-probe measurement for the first time experimental study whether ionizing radiation can produce modulations of optical properties, which can be utilized as a novel method for radiation detection.
            
            [<i>Papers:</i> 
               	<a href="https://iopscience.iop.org/article/10.1088/1361-6560/ab23cb/meta">PMB'19</a>,
               	<a href="https://ieeexplore.ieee.org/document/9059983">IEEE'20</a>,
                <a href="https://iopscience.iop.org/article/10.1088/1361-6560/abe027/meta">PMB'21</a>]
        </p>
        
	   </div>
    </td>
</tr>
<!-- ============================================================= -->
<tr padding-top="0">
    <td width="100%" valign="left" >	
        <div class="entry">
            <div>
                <img src="images/Reconstruction.png" border="0" height = 220pt class="img-1"/></a>
            </div>
        <p>
               <b> Penalized MLE image reconstruction for a dual-panel dedicated head and neck PET system:</b> Positron emission tomography (PET) suffers from limited spatial resolution in the current head and neck cancer management. We are building a dual-panel high-resolution PET system to aid the detection of tumor involvement in small lymph nodes (10 mm in diameter). The system is based on cadmium zinc Telluride (CZT) detectors with cross-strip electrode readout (1 mm anode pitch and 5 mm cathode pitch). In this work, we leverage a penalized maximum-likelihood (PML) reconstruction for the limited-angle PET system. The dissimilarity between the image to be reconstructed and a prior image from a low-resolution whole-body scanner is penalized. An image-based resolution model is incorporated into the regularization. The method studied in this work provides a way to mitigate the limited-angle artifacts in the reconstruction from limited-angle PET data.
            
            [<i>Papers:</i> 
               	<a href="https://iopscience.iop.org/article/10.1088/1361-6560/ab8c92">PMB'20</a>;
            <i> Code:</i> 
                <a href="https://github.com/YuliWanghust/RILRECON">RILRECON</a>
            ]
        </p>

	   </div>
    </td>
</tr>
<!-- ============================================================= -->
<tr padding-top="0">
    <td width="100%" valign="left" >	
        <div class="entry">
            <div>
                <img src="images/small_animal.png" border="0" height = 220pt class="img-1"/></a>
            </div>
        <p>
               <b> Automated location detection of injection site for preclinical stereotactic neurosurgery through fully convolutional networks:</b> Currently, injection sites of probes, cannula, and optic fibers in stereotactic neurosurgery are typically located manually. This step involves location estimations based on human experiences and thus introduces errors. In order to reduce location error and improve the repeatability of experiments and treatments, we have developed an automated locating framework to locating injection sites. This framework integrates a regional convolutional network and a fully convolutional network to locate specific anatomical points on skulls of rodents. Experiment results show that the proposed locating framework is capable to identify and locate Bregma and Lambda points (two important anatomical reference points) in rodent skull images with mean errors less than 250 um. This method is robust to different lighting conditions and mouse orientations.
            
            [<i>Papers:</i> 
               	Paper: <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0244378">Plos one'20</a>
            ]
        </p>

	   </div>
    </td>
</tr>
<!-- ============================================================= -->